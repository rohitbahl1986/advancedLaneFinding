{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Detection of lane markings plans a central role in a self driving car since this information is used place the car on the road safely. The object of this project then is to detect and track these lane markings in varying conditions (with shadows, broken/ continuous lanes, curved roads etc)  \n",
    "As an output of the pipeline, lane markings should be clearly marked in the output image/video. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Pipeline\n",
    "\n",
    "The pipeline for finding and marking lane lines is divided into 13 stages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera Calibration - Stage-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Camera calibration is the processing of images. This step is carried out once in the begining and resulting camera and distortion matrix are stored on the disk for later use.\n",
    "\n",
    "To calibrate the camera for distortion, I first use the open CV function **findChessboardCorners** to detect the corners in a chess board (a chess board is ideal for calibration since it has a highly regular pattern, making it easy for automatic detection) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ret, corners = cv2.findChessboardCorners(gray, (chessboard_size[0], chessboard_size[1]), None)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detected corners (image points) are then fed to the the function **calibrateCamera()** , which calculates the distortion and camera matrix :\n",
    "\n",
    "'ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(object_points, image_points, img_size, None,None)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the function **undistort()** is invoked to generated the undistored image.\n",
    "\n",
    "`dst = cv2.undistort(img_test, mtx, dist, None, mtx)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![undistortChess](camera_cal/test_undist.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![undistortRoad](test_images_output/undistorted_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Blur - Stage 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to smooth the input image and remove some amount of noise, I applied the Gaussian Blur operation with a kernel size of 3. I experimented with higher sizes as well without any significant improvement in the results (although making the kernel very high would eventually make the performance worse since it will be harder to detect the edges).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color /Gradient Threshold and Masking- Stage 3-8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cells [5] to [10], I defined functions for reducing the image down to just lane markings using various threshold methods.\n",
    "\n",
    "To optimally filter the images with color, I examined the images in a MS Paint to find out the RGB and HLS ranges for yellow and white. Based on this observation of minimum and maximum values of the RGB and HLS components I came up with the following values:\n",
    "\n",
    "Yellow HLS lower: [20,140,85]\n",
    "Yellow HLS upper: [35,180,245]\n",
    "\n",
    "While RGB lower : [200, 200, 200]\n",
    "While RGB upper = [255, 255, 255]\n",
    "\n",
    "Yellow RGB lower: [200,90,50]\n",
    "Yellow RGB upper: [255,220,140]\n",
    "\n",
    "After filtering the images with the above thresholds, they are converted to gray scale which reduces the number of channels.\n",
    "Finally all the 3 images are cascaded together to result in a single image with yellow and white color markings.\n",
    "\n",
    "To ensure that only the lane markings are present in the final image, I defined a region of interest in terms of a trapezoid. \n",
    "This helps to reduce the image and restrict it to lane markings only.\n",
    "\n",
    "Filtering by color alone is not robust enough to give good confidence in the quality of the detected lanes. \n",
    "Therefore, next I applied the sobel operator in the X and Y directions, on the magnitude of the gradient vector and on the direction of the gradient.\n",
    "**Before applying these gradient, I extracted the Saturation channel since I observed that gradient produced much better results when filtered with just the saturation channel**\n",
    "\n",
    "For determining the optimal threshold values, I restricted (held constant) 3 of the gradients and varied only the forth. After some iterations, I was able to find the values which worked well on the test data set.\n",
    "\n",
    "All the gradient images are then combined to provide a single binary image which is then combined with the output of the color threshold stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![undistortRoad](test_images_output/masked_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![undistortRoad](test_images_output/masked_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![undistortRoad](test_images_output/masked_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perspective Transform - Stage 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perspective transform is defined in cell [11]. I was not able to figure out an automated method for finding the source and destination points (which would have been optimal). Therefore, I resorted back to simply eye balling the source image and finding 4 points which defined a trapezoid, similar to the region of interest. The destination points are also defined by simply eye balling. I did have to spend significant amount of time though to get the coordinates right, which is extremely critical to get the right transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![undistortRoad](test_images_output/wrapped_1.png)\n",
    "![undistortRoad](test_images_output/wrapped_2.png)\n",
    "![undistortRoad](test_images_output/wrapped_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifyling and Drawing Lanes - Stage 10/11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell [12] defines the **sliding window** algorithm to identify the pixels which constitute the left and right lanes.\n",
    "As a first step, the entire image is divided into several windows. We start the search for the lane lines by plotting a histogram of bottom half of the image. Two peaks are then identified which form the base for starting the search for the lanes. \n",
    "In each step we search in a small window for a certian number of pixels. In each subsequent step the base of the window is moved to track the mean of the pixels found. This helps in trackign the lane lines.\n",
    "\n",
    "A polynomial of order 2 is then fitted and the coefficents are then used to generate the fitted x co-ordinates. I tried higher order polynomials as well, but order 2 gave the best fit.\n",
    "\n",
    "I did not get a chance to implement the look ahead filter though because of the which the pipeleine bresk in the challange video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![undistortRoad](test_images_output/histogram_1.png)\n",
    "![undistortRoad](test_images_output/lanes_1.png)\n",
    "\n",
    "![undistortRoad](test_images_output/histogram_2.png)\n",
    "![undistortRoad](test_images_output/lanes_2.png)\n",
    "\n",
    "![undistortRoad](test_images_output/histogram_3.png)\n",
    "![undistortRoad](test_images_output/lanes_3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radius of Curvature and Lane Offset - Stage 12/13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, I calculated and plotted the radius of curvature of the lane and the offset of the care from the center of the image (cell [14] and [15]) . \n",
    "\n",
    "**NOTE** : Radius of curvature seems reasonable for most of the video ( < 2 KM). However, it is seen to juct to very large values when the road becomes relatevly stragight, which is then should be the expected result as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![undistortRoad](test_images_output/final_1.png)\n",
    "![undistortRoad](test_images_output/final_2.png)\n",
    "![undistortRoad](test_images_output/final_3.png)\n",
    "![undistortRoad](test_images_output/final_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disscussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For me the tricky part of this project was to get the various tune able parameters right. \n",
    "There are lot of knobs in the pipeline which can be tuned up or down to get the desired results and it is critical to get all of them right. For ex, we can play around with the kernel size of the Gaussian blur and Sobel operator, threshold values of the color being filtered, color spaces to select in which we should carry out the filtering, threshold for the gradients and the region of interest. \n",
    "However, in the absence of a programmable way, selecting the right source and destination points for the perspective transform was the most time consuming part of the project. As I learned from experimentation, that it is critical to get these right else the wrapped image can be all over the place.\n",
    "\n",
    "While the pipeline does well on the project video and fails in the challenge video. \n",
    "The reason for this is the absence of a look ahead filter and sanity check. A sanity check function would detect if lanes were found or not and if they are not found would then extrapolate lines from the previous frame, making the algorithm more robust.\n",
    "Essentially, the pipeline is likely to break when the lanes are quite dark say at night and while passing under a bridge (where the shadow will be quite dominant)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
